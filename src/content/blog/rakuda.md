---
title: "Ranking Japanese LLMs"
meta_title: "Introducing the Rakuda Benchmark"
description: "Introducing the Rakuda Benchmark"
date: 2023-06-23T05:00:00Z
image: "/images/image-placeholder.png"
categories: ["LLM"]
author: "Sam Passaglia"
tags: ["blog", "japanese", "machine learning", "yuzu-ai"]
draft: false
---

DRAFT 6/24/13:59pm

```
{note}
This post also appears on the [YuzuAI blog](https://yuzuai.jp/), and all of the code and data used to get these results is now available on [github](https://github.com/yuzu-ai/japanese-llm-ranking).
```

The open-source community has been hard at work trying to catch up to closed Large Language Models like ChatGPT. Open models --- models whose insides are released publicly --- are important because they enable AI research and development outside of the control of large corporations.

In English, the best open models are now at roughly the level of the GPT-3.5 model that powers the free tier of ChatGPT. The Japanese performance of open models seems to lag behind, however, and the Japanese open-source community is not as active as the community in the West. Nonetheless the past couple months have seen the release of a [3.6-billion parameter model](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-ppo) by [rinna](https://rinna.co.jp/) and the 7-billion parameter [open-calm](https://huggingface.co/cyberagent/open-calm-7b) model by [CyberAgent](https://www.cyberagent.co.jp/), both trained exclusively on Japanese text.

How good are open-source models in Japanese, and how do they compare to the [strong Japanese performance of ChatGPT](https://passaglia.jp/gpt-japanese/)? And within the open-source space, how do large models trained primarily in English compare to models which are smaller but which focus their training and architectures on Japanese?

To answer these questions, we introduce here the **Rakuda** benchmark and ranking for Japanese AI assistants. Read on for details about how it works, or head directly to [the leaderboard](https://yuzuai.jp/benchmark) to see how the first 6 competitors stack up!

# The Rakuda Benchmark

One powerful way to test an LLM is to ask it questions to which you know the answer. This is the approach taken by many popular benchmarks like the Japanese General Language Understanding Evaluation ([JGLUE](https://github.com/yahoojapan/JGLUE)) or the Massive Multitask Language Understanding ([MMLU](https://github.com/hendrycks/test)) test. In order to measure a model's performance on these questions unambiguously, these benchmarks are often designed as multiple choice tests.

But LLMs are capable of much more than answering multiple choice questions. ChatGPT and similar models can write detailed, reasoned, and pertinent responses to open-ended questions without clear right or wrong answers. The performance of these AI assistants is defined by how useful they are to their users, and we would like to measure that usefulness.

One way to do that is to simply ask users directly. This is the approach taken by LMSYS's [Chatbot Arena](https://lmsys.org/blog/2023-05-03-arena/). They provide a [web interface](https://chat.lmsys.org/?arena) where users can input questions, just like they would to ChatGPT, and LMSYS serves them back answers generated by two different AI assistants. The user is asked to decide which answer is better, and based on users' preferences LMSYS creates an [LLM leaderboard](https://chat.lmsys.org/?leaderboard).

![A ranking of AI assistants from https://chat.lmsys.org/?leaderboard](./src/posts/2023-06-25-rakuda-benchmark/lmsys_leaderboard.png)

We'll talk more about just how to create such a ranking down below, but in the meantime it's important to understand that this approach is in some ways very easy, because you don't need to come up with a list of questions and answers yourself, and in other ways hard, because you have to serve models reliably and quickly to users on-demand.

A simpler approach is to come up with a set of questions yourself and then judge models' responses to these questions against each other. Since the question list is fixed, you can generate all the answers at once and you don't have to serve any models to users over the internet. You still don't need to specify answers to the questions, because you'll just compare models' responses against each other rather than against some ground truth. And the questions you use can be open-ended and admit a wide range of possible answers, and they can be designed to test exactly the skills you're interesting in.

Benchmarks and rankings based on open-ended question lists have proven incredibly useful in stimulating open-source development. Benchmarks based on the [Vicuna list](https://lmsys.org/vicuna_eval/) are used in many papers, and Huggingface uses such a list to produce its [LLM assistant leaderboard](https://huggingface.co/spaces/HuggingFaceH4/human_eval_llm_leaderboard). But so far there has been no such a list for Japanese.

That's why we designed **Rakuda**, the first question set designed to evaluate how well AI Assistants can solve Japanese-specific tasks, and we're releasing it to the public as a [dataset on Huggingface](https://huggingface.co/datasets/yuzuai/rakuda-questions).

**Rakuda** contains 40 questions in Japanese about Japanese history, society, government, and geography. Questions in the first three categories are open-ended, while the geography questions are more specific. Here's an example question from each category

```
## Rakuda questions

歴史： 幕末に西洋と接触した結果、日本で生じた社会変革について説明してください。
# History: Explain the societal transformations that occurred in Japan
#          as a result of contact with the West during the Bakumatsu.

社会： 日本の労働市場の女性活躍推進に関する政策とその結果について説明してください。
# Society: Explain the policies that have been implemented to promote
#          the success of women in the Japanese workplace,
#          and those policies' results.

政治： 日本の議院内閣制度を説明し、そのメリットとデメリットを詳述してください。
# Government: Explain Japan's parliamentary system of government,
#             and its merits and demerits.

地理： 熊野古道は、日本のどの地方に位置し、主に何のために使われていましたか。
# Geography: Where is the Kumano Kodō,
#            and what was it primarily used for?
```

We got these questions by asking ChatGPT-4 to generate useful questions to evaluate Japanese high-school students. We kept the ChatGPT-4 outputs we liked, removed questions we thought were bad, and lightly edited some for clarity.

# Evaluating and Ranking Models on Rakuda

**Rakuda** does not contain answers to the questions, which are designed to be open ended and admit many possible answers. Models are evaluated by generating answers to the questions and then comparing their answers against each other.

Ideally we would get humans reviewers to do these comparisons, looking at two models' outputs to the same question and deciding which is better. But doing all pairwise combinations for 6 LLMs on 40 questions would take 6x5x40 = 1200 reviews (we want to compare model answers in both A-B and B-A orderings to eliminate any primacy bias of the reviewers). That's a lot of work, so instead we ask the best LLM we have access to, GPT-3.5, to do the reviewing. We can do this because [a recent paper](https://arxiv.org/abs/2306.05685) showed that reviews by GPT-3.5 agree with human reviewers 83% of the time. GPT-4 does even better, so please contact us if you have API access to GPT-4 and would be willing to help us use it to improve our benchmark.

We send GPT-3 a question from the Rakuda list and two different answers to it and ask it to choose which is better. We also allow it to choose a draw, but it doesn't do so very often. We do this for all possible model match-ups for every question and end up with a long table that looks like

```bash
model_1    | model_2   | winner
------------------------------
gpt-3      | open-calm | 1
rinna-3.6b | gpt-3     | 2
rinna-3.6b | open-calm | 2
..
(1200 rows)
```

The distribution of reviews is...

TODO: PLOT HERE OF REVIEW DISTRIBUTION

and we see that the reviewer (GPT-3) has a slight bias towards picking the second answer that it is shown. Since every model gets shown just as often in position 1 as in position 2, this won't bias our results.

Once we have this list of matches and results, we want to make a ranking based on which models our reviewer prefers. By making a ranking we are assuming that our reviewer's preferences define a [total ordering](https://en.wikipedia.org/wiki/Total_order) on the set of model outputs. In particular we assume that match results are distributed according to the [Bradley-Terry model](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model).

The Bradley-Terry model is a way of modeling paired competitions. It says that every competitor $$i$$ (in our case a Japanese LLM) is defined by a single **strength** parameter $$\beta_i$$, and the probability of
victory of competitor $$i$$ in a match against competitor $$j$$ just depends on their relative strength $$\beta_i - \beta_j$$. In particular a commonly used form for the victory probability is

$$ p\_{ij} \equiv \textrm{Probability} ( i \textrm{ beats } j) = \dfrac{e^{\alpha + \beta_i - \beta_j}}{1 + e^{\alpha + \beta_i - \beta_j} } $$,

where $$\alpha$$ is a home-field advantage parameter that enhances model $$i$$'s strength. To use it we always order match pairs $$(i, j)$$ with $$i$$ as the home team.

We can define our data vector $$\vec{Y}$$ as a vector that contains an entry for each match: 0 if the home team loses the match or 1 if the home team wins. Then the probability of getting our exact data-vector given a set of model parameters $$(\alpha, \vec{\beta})$$ is just the product over every match of $$p_{ij}$$ if the home-team model won the match, or $$(1-p_{ij})$$ if the home team lost. This is the likelihood

$$ \textrm{Likelihood}(\alpha, \vec{\beta}) = \prod*{\textrm{matches}} p*{ij}^Y \times (1-p\_{ij})^{1-Y} $$

The best fit parameters -- including the best fit model strengths -- are those which maximize the likelihood. This maximization is very easy to do in python with scipy's optimize function. A couple technical points: first, we deal with draws by counting them as a half-win for each team. Second, because winning probabilities are controlled only by relative model strengths, a constant can be added or subtracted to all model strengths without changing the likelihood. We fix this gauge freedom by imposing that the model strengths all add up to zero.

After getting the best-fit model strengths, we want to know their uncertainties. There are many ways to do this, but the one that requires the lowest brain power is certainly [Markov chain Monte Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo). In our Bayesian framework, the uncertainties are related to the shape of the posterior probability distribution of the parameters, which is proportional to the likelihood times the prior probability of the parameters. MCMC is an easy way to sample points from the posterior, and how far the samples stray from the best-fit points tells us the parameter uncertainties. This all might sound a bit complicated, but it's very straightforward to implement using the [emcee](https://emcee.readthedocs.io/en/stable/tutorials/line/) package.

Finally, a note about Elo scores. Elo scores are another commonly used system to rank the relative skill of players in zero-sum competitions. In fact, they are based on the same underlying Bradley-Terry model we use here. The difference lies only in how the player strengths are actually computed from the data. In the Elo system, player's scores are gradually updated as they play matches based on their pre-match Elo score and the Elo score of their opponents. This is popular with players and spectators because they can easily calculate the exact impact a win or loss will have on their score. But from a parameter estimation standpoint it is sub-optimal and will only slowly approach the maximum likelihood values. If you have access to a computer, better to take the Bayesian approach and fit all strengths and all matches at once as we've done here.

# The Rakuda Leaderboard

For our initial release we include 6 models

- GPT-3.5 (OpenAI): This is the best model in the ranking and also the model we use as a reviewer. We might worry that as a reviewer GPT-3.5 is biased towards its own
- Open-Calm and stormy
- Rinna-ppo, sft-v2, and base.

All data - generations, reviews, etc is on the github. The home-field advantage parameter is ... which means that if two teams of the same strength face each other the one presented first to the reviewer will win X percent of the time.

The best fit parameters are

TODO: Plot here of parameters

The uncertainties are correlated, but with MCMC chains we can make statements like ...

TODO: Plot here explaining the uncertainty: grid or just compare models to next model in ranking

Discussion about future plans...

If you have any models you'll like us to add just let us know.

but they will continuously updated on the main page https://yuzuai.jp/benchmark .
